# test_pipeline.py
import json
from pathlib import Path
import numpy as np
import re
from sentence_transformers import SentenceTransformer
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


# ========= é…ç½®åŒº =========

# å·²æ ¼å¼åŒ–çš„æ–°æŒ‡ä»¤
NEW_QUERY_JSON = "new_query_format.json"   # { "sentences": [ ... ] }

# è€æŒ‡ä»¤ï¼ˆè‡ªç„¶è¯­è¨€ + å‘é‡ï¼‰
OLD_QUERY_JSON = "old_query.json"         # { "sentences": [ ... ] }
OLD_QUERY_NPY  = "old_query.npy"          # shape = (N, D)

# ç›¸ä¼¼åº¦é˜ˆå€¼ & topk
SIM_THRESHOLD = 0.7
TOP_K = 5

# GTE æ¨¡å‹åç§°
GTE_MODEL_NAME = "/root/autodl-tmp/gte"

# Qwen3-8B æœ¬åœ°æ¨¡å‹ç›®å½•
QWEN_MODEL_DIR = "/root/autodl-tmp/Qwen3-8B"


# ========= å·¥å…·å‡½æ•° =========

def load_sentences(path: str):
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    sents = data.get("sentences", [])
    if not sents:
        raise ValueError(f"{path} ä¸­æœªæ‰¾åˆ° 'sentences' æˆ–åˆ—è¡¨ä¸ºç©º")
    return sents


def load_old_data():
    """åŠ è½½è€æŒ‡ä»¤çš„è‡ªç„¶è¯­è¨€å’Œ npy å‘é‡"""
    sentences = load_sentences(OLD_QUERY_JSON)
    embeddings = np.load(OLD_QUERY_NPY)
    if embeddings.shape[0] != len(sentences):
        raise ValueError("old_query.json ä¸­å¥å­æ•°ä¸ old_query.npy è¡Œæ•°ä¸ä¸€è‡´")
    return sentences, embeddings


def cosine_sim(new_vec: np.ndarray, old_vecs: np.ndarray) -> np.ndarray:
    """new_vec: (D,), old_vecs: (N, D)"""
    new_norm = new_vec / (np.linalg.norm(new_vec) + 1e-12)
    old_norm = old_vecs / (np.linalg.norm(old_vecs, axis=1, keepdims=True) + 1e-12)
    return old_norm @ new_norm   # (N,)


class QwenSelector:
    """ç”¨ Qwen3-8B åœ¨å€™é€‰ä¸­é€‰å‡ºæœ€ç›¸ä¼¼çš„ä¸€æ¡"""

    def __init__(self, model_dir: str):
        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            device_map="auto",
            torch_dtype="auto",
            use_safetensors=True,
        )
        self.model.eval()

    def select_best(self, new_instruction: str, candidates: list[str]) -> int:
        """
        è¿”å›å€™é€‰åˆ—è¡¨ä¸­çš„ indexï¼ˆ0-basedï¼‰
        """
        numbered = "".join([f"{i+1}.{c}" for i, c in enumerate(candidates)])
        system_content = (
            "ä½ æ˜¯ä¸€ä¸ªè¯­ä¹‰ç›¸ä¼¼åº¦åˆ¤åˆ«å·¥å…·ï¼Œç°åœ¨æœ‰è¿™å‡ æ¡è€æŒ‡ä»¤ï¼š" +
            numbered +
            "ï¼Œç»™ä½ ä¸€æ¡æ–°æŒ‡ä»¤ï¼Œä»è€æŒ‡ä»¤ä¸­æ‰¾å‡ºè¯­ä¹‰æœ€ç›¸ä¼¼çš„ä¸€æ¡ï¼Œ"
            "åªè¾“å‡ºè¯¥æŒ‡ä»¤çš„ç¼–å·ï¼ˆé˜¿æ‹‰ä¼¯æ•°å­—ï¼‰ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šå’Œå¤è¿°ã€‚"
        )

        messages = [
            {"role": "system", "content": system_content},
            {"role": "user",   "content": f"æ–°æŒ‡ä»¤ï¼š{new_instruction}"}
        ]

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
            enable_thinking=False,
        )

        model_inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        with torch.no_grad():
            gen_ids = self.model.generate(
                **model_inputs,
                max_new_tokens=16,
            )
        gen_ids = [
            output_ids[len(input_ids):]
            for input_ids, output_ids in zip(model_inputs.input_ids, gen_ids)
        ]
        resp = self.tokenizer.batch_decode(gen_ids, skip_special_tokens=True)[0]

        m = re.search(r"\d+", resp)
        if not m:
            raise RuntimeError(f"Qwen è¾“å‡ºä¸­æœªæ‰¾åˆ°ç¼–å·: {resp}")
        idx = int(m.group()) - 1
        if not (0 <= idx < len(candidates)):
            raise RuntimeError(f"Qwen è¾“å‡ºç¼–å·è¶Šç•Œ: {resp} (len={len(candidates)})")
        return idx


# ========= ä¸»æµ‹è¯•æµç¨‹ =========

def main():
    # 1. æ£€æŸ¥æ–‡ä»¶
    if not Path(NEW_QUERY_JSON).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {NEW_QUERY_JSON}")
    if not Path(OLD_QUERY_JSON).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {OLD_QUERY_JSON}")
    if not Path(OLD_QUERY_NPY).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {OLD_QUERY_NPY}")

    # 2. åŠ è½½æ•°æ®
    new_sents = load_sentences(NEW_QUERY_JSON)
    old_sents, old_embs = load_old_data()

    print(f"æ–°æŒ‡ä»¤æ•°é‡: {len(new_sents)}")
    print(f"è€æŒ‡ä»¤æ•°é‡: {len(old_sents)}")

    # 3. åŠ è½½æ¨¡å‹
    print("\n[åŠ è½½ GTE æ¨¡å‹] ...")
    embedder = SentenceTransformer(GTE_MODEL_NAME)

    print("[åŠ è½½ Qwen3-8B] ...")
    qwen = QwenSelector(QWEN_MODEL_DIR)

    total = len(new_sents)
    success = 0
    mismatches = []

    # 4. é€æ¡æµ‹è¯•
    for i, new_ins in enumerate(new_sents):
        expected_index = i + 1  # new_query_format ä¸­çš„åºå·ï¼ˆä»1å¼€å§‹ï¼‰
        print("\n" + "=" * 60)
        print(f"æµ‹è¯•æ ·æœ¬ #{expected_index}")
        print("æ–°æŒ‡ä»¤ï¼š", new_ins)

        # 4.1 ç¼–ç æ–°æŒ‡ä»¤
        new_vec = embedder.encode(new_ins, convert_to_numpy=True)

        # 4.2 è®¡ç®—ä¸è€æŒ‡ä»¤çš„ç›¸ä¼¼åº¦
        sims = cosine_sim(new_vec, old_embs)
        sorted_idx = np.argsort(-sims)
        top_idx = sorted_idx[:TOP_K]
        top_scores = sims[top_idx]

        print("Top5 ç›¸ä¼¼åº¦ï¼š")
        for rank, (idx0, sc) in enumerate(zip(top_idx, top_scores), 1):
            print(f"  æ’å{rank}: å…¨å±€åºå·{idx0+1}, ç›¸ä¼¼åº¦={sc:.4f}, æŒ‡ä»¤={old_sents[idx0]}")

        # 4.3 é˜ˆå€¼åˆ¤æ–­
        if top_scores[0] <= SIM_THRESHOLD:
            print("  â†’ æœ€é«˜ç›¸ä¼¼åº¦<=é˜ˆå€¼ï¼Œåˆ¤å®šä¸ºæ— å¯å¤ç”¨ä»»åŠ¡")
            mismatches.append({
                "case_id": expected_index,
                "query": new_ins,
                "expected": expected_index,
                "got": None,
                "reason": "no_match",
            })
            continue

        # 4.4 ä» topK ä¸­é€‰å‡º > é˜ˆå€¼ çš„å€™é€‰
        cand_global_idx = [idx0 for idx0, sc in zip(top_idx, top_scores) if sc > SIM_THRESHOLD]
        cand_texts = [old_sents[idx0] for idx0 in cand_global_idx]

        print("\nè¶…è¿‡é˜ˆå€¼çš„å€™é€‰ï¼š")
        for j, (gidx, txt) in enumerate(zip(cand_global_idx, cand_texts), 1):
            print(f"  å±€éƒ¨{j} -> å…¨å±€{gidx+1}, æŒ‡ä»¤ï¼š{txt}")

        # ç†è®ºä¸Š cand_global_idx è‡³å°‘æœ‰ä¸€ä¸ªï¼ˆå› ä¸º top1>é˜ˆå€¼ï¼‰ï¼Œä½†ä¿é™©èµ·è§å†åˆ¤æ–­ä¸€ä¸‹ï¼š
        if not cand_global_idx:
            print("  â†’ å‡ºç°å¼‚å¸¸ï¼štop1>é˜ˆå€¼ä½†å€™é€‰ä¸ºç©º")
            mismatches.append({
                "case_id": expected_index,
                "query": new_ins,
                "expected": expected_index,
                "got": None,
                "reason": "empty_candidates",
            })
            continue

        # 4.5 ç”¨ Qwen åœ¨å€™é€‰ä¸­åšæœ€ç»ˆåˆ¤æ–­
        local_idx = qwen.select_best(new_ins, cand_texts)
        global_idx0 = cand_global_idx[local_idx]  # 0-based
        predicted_index = global_idx0 + 1         # å¯¹å¤–ä»1å¼€å§‹

        if predicted_index == expected_index:
            print(f"\nâœ… åŒ¹é…æˆåŠŸï¼šæœŸæœ›åºå· = {expected_index}, å®é™…åºå· = {predicted_index}")
            success += 1
        else:
            print(f"\nâŒ åŒ¹é…å¤±è´¥ï¼šæœŸæœ›åºå· = {expected_index}, å®é™…åºå· = {predicted_index}")
            mismatches.append({
                "case_id": expected_index,
                "query": new_ins,
                "expected": expected_index,
                "got": predicted_index,
                "reason": "wrong_index",
            })

    # 5. æ±‡æ€»
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•å®Œæˆï¼šé€šè¿‡ {success}/{total} æ¡")

    if mismatches:
        print("\nä»¥ä¸‹æ ·æœ¬ä¸ºå¼‚å¸¸ï¼ˆåŒ…æ‹¬æ— åŒ¹é…ã€åŒ¹é…é”™åºå·ç­‰ï¼‰ï¼š")
        for m in mismatches:
            print("-" * 40)
            print(f"æ ·æœ¬åºå·ï¼ˆnew_query_format.json ä¸­ï¼Œä»1å¼€å§‹ï¼‰ï¼š{m['case_id']}")
            print(f"  æŒ‡ä»¤ï¼š{m['query']}")
            print(f"  æœŸæœ›è€æŒ‡ä»¤åºå·ï¼š{m['expected']}")
            print(f"  å®é™…è€æŒ‡ä»¤åºå·ï¼š{m['got']}")
            reason = m["reason"]
            if reason == "no_match":
                print("  å¼‚å¸¸åŸå› ï¼šæœ€é«˜ç›¸ä¼¼åº¦<=é˜ˆå€¼ï¼Œè¢«åˆ¤å®šä¸ºæ— å¯å¤ç”¨ä»»åŠ¡")
            elif reason == "wrong_index":
                print("  å¼‚å¸¸åŸå› ï¼šé€‰ä¸­çš„è€æŒ‡ä»¤åºå·ä¸æœŸæœ›ä¸ä¸€è‡´")
            elif reason == "empty_candidates":
                print("  å¼‚å¸¸åŸå› ï¼šé€»è¾‘å¼‚å¸¸ï¼ˆtop1>é˜ˆå€¼ä½†å€™é€‰åˆ—è¡¨ä¸ºç©ºï¼‰")
            else:
                print(f"  å¼‚å¸¸åŸå› ï¼š{reason}")
    else:
        print("ğŸ‰ æ‰€æœ‰æ ·æœ¬éƒ½åŒ¹é…åˆ°äº†æ­£ç¡®åºå·ï¼")


if __name__ == "__main__":
    main()
